{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4d8a10a-fe1e-4e8d-9775-ec7c120d9e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from PIL import Image\n",
    "# from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "082e8e06-c7cf-4036-a65f-72e9edf91f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "beb42d83-e645-4540-a674-df78ca3e1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer # For BiomedCLIP model\n",
    "from transformers import AutoTokenizer, AutoModel # For BioLinkBERT model\n",
    "from sentence_transformers import SentenceTransformer # For MPNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb9fa37c-7304-4b35-8ada-f84fb8345f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "biomedclip_model, preprocess = create_model_from_pretrained('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "biomedclip_model.to(device)\n",
    "tokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dbea964-eac3-4ba3-8dad-145b1223c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"largeListsGuy/gpt4o_response_list.json\", 'r') as f:\n",
    "    gpt4o_response_list = json.load(f)\n",
    "\n",
    "gpt4o_dict = {idx: response for idx, response in enumerate(gpt4o_response_list) if not \"Other/None/Unknown\" in response}\n",
    "\n",
    "filtered_csv_path = r'/cs/labs/tomhope/dhtandguy21/envs/pmc_project/21_sep_filtered_data.csv'\n",
    "filtered_df = pd.read_csv(filtered_csv_path)\n",
    "filtered_df.loc[list(gpt4o_dict.keys()), \"gpt4o_response\"] = list(gpt4o_dict.values())\n",
    "def normalize(text):\n",
    "    return re.sub(r'\\s+', ' ', text.strip())\n",
    "    \n",
    "filtered_df[\"gpt4o_response\"] = filtered_df[\"gpt4o_response\"].apply(normalize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8980ccad-e0b1-4e84-9205-f927526b08b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"largeListsGuy/retrieval_labeled_img_pairs.pkl\", \"rb\") as f:\n",
    "    labeled_img_pairs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55b7e486-a384-4b60-86dc-4b758434fb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1321"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled_img_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b872e6-7604-4716-8f2e-6fb9ef1668a8",
   "metadata": {},
   "source": [
    "In the next few cells, we will split the pairs into training and test set.\n",
    "\n",
    "In order to prevent data leakage, it is necessary that images corresponding to the same patient will not appear in both training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed85bfe8-a777-48cb-931a-cfebf84301b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1056 (79.94%)\n",
      "Test samples: 265 (20.06%)\n",
      "Discarded samples: 0\n",
      "UID exclusivity between training and test sets is maintained.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "def extract_uid(img_path):\n",
    "    \"\"\"\n",
    "    Extracts the uid from the image path.\n",
    "    Assumes that the uid is the last directory before the image filename.\n",
    "    \"\"\"\n",
    "    # Split the path into parts\n",
    "    path_parts = os.path.normpath(img_path).split(os.sep)\n",
    "    # Get the uid (second last part)\n",
    "    uid = path_parts[-2]\n",
    "    return uid\n",
    "\n",
    "def dfs(uid, visited, component):\n",
    "    visited.add(uid)\n",
    "    component.add(uid)\n",
    "    for neighbor in uid_graph[uid]:\n",
    "        if neighbor not in visited:\n",
    "            dfs(neighbor, visited, component)\n",
    "\n",
    "# Build the uid_graph\n",
    "uid_graph = defaultdict(set)\n",
    "\n",
    "# Build the graph\n",
    "for (img_path1, img_path2), label in labeled_img_pairs:\n",
    "    uid1 = extract_uid(img_path1)\n",
    "    uid2 = extract_uid(img_path2)\n",
    "    uid_graph[uid1].add(uid2)\n",
    "    uid_graph[uid2].add(uid1)\n",
    "\n",
    "# Find connected components\n",
    "visited = set()\n",
    "components = []\n",
    "\n",
    "for uid in uid_graph:\n",
    "    if uid not in visited:\n",
    "        component = set()\n",
    "        dfs(uid, visited, component)\n",
    "        components.append(component)\n",
    "\n",
    "# Step 1: Build UID to component index mapping\n",
    "uid_to_component_idx = {}\n",
    "\n",
    "for idx, component in enumerate(components):\n",
    "    for uid in component:\n",
    "        uid_to_component_idx[uid] = idx\n",
    "\n",
    "# Step 2: Count samples per component\n",
    "component_sample_counts = [0] * len(components)\n",
    "\n",
    "for (img_path1, img_path2), label in labeled_img_pairs:\n",
    "    uid1 = extract_uid(img_path1)\n",
    "    component_idx = uid_to_component_idx[uid1]\n",
    "    component_sample_counts[component_idx] += 1\n",
    "\n",
    "# Step 3: Sort components by sample count\n",
    "components_with_counts = list(zip(components, component_sample_counts))\n",
    "components_with_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Step 4: Assign components to training and test sets\n",
    "total_samples = len(labeled_img_pairs)\n",
    "desired_train_samples = int(total_samples * 0.8)\n",
    "\n",
    "train_uids = set()\n",
    "test_uids = set()\n",
    "\n",
    "accumulated_train_samples = 0\n",
    "\n",
    "for component, sample_count in components_with_counts:\n",
    "    if accumulated_train_samples < desired_train_samples:\n",
    "        train_uids.update(component)\n",
    "        accumulated_train_samples += sample_count\n",
    "    else:\n",
    "        test_uids.update(component)\n",
    "\n",
    "# Step 5: Assign samples to training and test sets, discard cross-set samples\n",
    "train_data = []\n",
    "test_data = []\n",
    "discarded_samples = []\n",
    "\n",
    "for sample in labeled_img_pairs:\n",
    "    (img_path1, img_path2), label = sample\n",
    "    uid1 = extract_uid(img_path1)\n",
    "    uid2 = extract_uid(img_path2)\n",
    "\n",
    "    if uid1 in train_uids and uid2 in train_uids:\n",
    "        train_data.append(sample)\n",
    "    elif uid1 in test_uids and uid2 in test_uids:\n",
    "        test_data.append(sample)\n",
    "    else:\n",
    "        # Discard cross-set samples to maintain UID exclusivity\n",
    "        discarded_samples.append(sample)\n",
    "\n",
    "# Step 6: Verify the split ratio\n",
    "train_sample_count = len(train_data)\n",
    "test_sample_count = len(test_data)\n",
    "total_sample_count = train_sample_count + test_sample_count\n",
    "\n",
    "train_ratio = train_sample_count / total_sample_count\n",
    "test_ratio = test_sample_count / total_sample_count\n",
    "\n",
    "print(f\"Training samples: {train_sample_count} ({train_ratio:.2%})\")\n",
    "print(f\"Test samples: {test_sample_count} ({test_ratio:.2%})\")\n",
    "print(f\"Discarded samples: {len(discarded_samples)}\")\n",
    "\n",
    "# Step 7: Ensure UID exclusivity in the test set\n",
    "uids_in_training_samples = set()\n",
    "for (img_path1, img_path2), label in train_data:\n",
    "    uids_in_training_samples.update([extract_uid(img_path1), extract_uid(img_path2)])\n",
    "\n",
    "uids_in_test_samples = set()\n",
    "for (img_path1, img_path2), label in test_data:\n",
    "    uids_in_test_samples.update([extract_uid(img_path1), extract_uid(img_path2)])\n",
    "\n",
    "overlap_uids = uids_in_test_samples.intersection(uids_in_training_samples)\n",
    "assert len(overlap_uids) == 0, \"Overlap detected between training and test UIDs!\"\n",
    "\n",
    "print(\"UID exclusivity between training and test sets is maintained.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "885b6548-cdee-4646-8c6c-99455f1e5b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cross-set samples: 0\n"
     ]
    }
   ],
   "source": [
    "# Count cross-set samples\n",
    "cross_set_sample_count = 0\n",
    "\n",
    "for sample in labeled_img_pairs:\n",
    "    (img_path1, img_path2), label = sample\n",
    "    uid1 = extract_uid(img_path1)\n",
    "    uid2 = extract_uid(img_path2)\n",
    "\n",
    "    if (uid1 in train_uids and uid2 in test_uids) or (uid1 in test_uids and uid2 in train_uids):\n",
    "        cross_set_sample_count += 1\n",
    "\n",
    "print(f\"Number of cross-set samples: {cross_set_sample_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14e83273-0a35-4fe1-8555-4dd3d036bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlap_uids = uids_in_test_samples.intersection(uids_in_training_samples)\n",
    "assert len(overlap_uids) == 0, \"Overlap detected between training and test UIDs!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c537dbad-d862-4052-8a01-0142661e67fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples accounted for: 1321\n",
      "Original total samples: 1321\n"
     ]
    }
   ],
   "source": [
    "total_samples_accounted = train_sample_count + test_sample_count + len(discarded_samples)\n",
    "original_total_samples = len(labeled_img_pairs)\n",
    "assert total_samples_accounted == original_total_samples, \"Mismatch in total sample count!\"\n",
    "\n",
    "print(f\"Total samples accounted for: {total_samples_accounted}\")\n",
    "print(f\"Original total samples: {original_total_samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c702c77c-dd2c-4bc6-9cc6-c3688d5ac448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1321\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for (img_pair, label) in labeled_img_pairs:\n",
    "    if label == 1:\n",
    "        counter += 1\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "545ef549-247c-4b43-8695-f38b8f456ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1056\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for (img_pair, label) in train_data:\n",
    "    if label == 1:\n",
    "        counter += 1\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "724af2e4-8f14-44c2-beea-0c7a3289b002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for (img_pair, label) in test_data:\n",
    "    if label == 1:\n",
    "        counter += 1\n",
    "print(counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f2381ac-629a-4516-b4bb-38986ede8164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define a set of augmentations\n",
    "positive_augmentations = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n",
    "    # Do not include ToTensor() or Normalize() here\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f360e352-88ad-41ab-9d6e-3a5bf54900dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class TestImagePairDataset(Dataset):\n",
    "    def __init__(self, pairs_list, transform=None):\n",
    "        self.pairs_list = pairs_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        (img_path1, img_path2), label = self.pairs_list[idx]\n",
    "        img1 = Image.open(img_path1).convert('RGB')\n",
    "        img2 = Image.open(img_path2).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return (img1, img2), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2b064f1-17d5-4dfe-b580-2b6336db83c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainImagePairDataset(Dataset):\n",
    "    def __init__(self, pairs_list, transform=None, augmentation=None):\n",
    "        self.pairs_list = pairs_list\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.expanded_pairs_list = self.expand_positive_pairs()\n",
    "        \n",
    "    def expand_positive_pairs(self):\n",
    "        expanded_list = []\n",
    "        for pair, label in self.pairs_list:\n",
    "            expanded_list.append(((pair[0], pair[1]), label))  # Original pair\n",
    "            if label == 1:\n",
    "                expanded_list.append(((pair[0], pair[1]), label))  # Augmented pair\n",
    "        return expanded_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.expanded_pairs_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        (img_path1, img_path2), label = self.expanded_pairs_list[idx]\n",
    "        img1 = Image.open(img_path1).convert('RGB')\n",
    "        img2 = Image.open(img_path2).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        # Apply augmentation to the augmented copies\n",
    "        if label == 1 and idx % 2 == 1:  # Every second positive pair is augmented\n",
    "            if self.augmentation:\n",
    "                img1 = self.augmentation(img1)\n",
    "                img2 = self.augmentation(img2)\n",
    "\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return (img1, img2), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43578c89-39e6-413c-9afb-d1e62977b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"largeListsGuy/visual_labeled_img_negative_pairs.pkl\", \"rb\") as f:\n",
    "    visual_labeled_img_negative_pairs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbda3baf-b843-41a0-81dd-521426f5cae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1727"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(visual_labeled_img_negative_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f07db348-0e34-40d3-94cd-27be0cb19b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.extend(visual_labeled_img_negative_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed04cec4-5ded-4627-b9ec-b9970d968416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2783"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7fa31d7-4c19-4ccc-9f7e-f99bbca4499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Dataloader for training set\n",
    "# train_dataset = TrainImagePairDataset(train_data, transform=preprocess, augmentation=positive_augmentations)\n",
    "train_dataset = TrainImagePairDataset(train_data, transform=preprocess, augmentation=None)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# Dataloader for test set\n",
    "test_dataset = TestImagePairDataset(test_data, transform=preprocess)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c3270d1-89ef-474f-9a53-73e9cd46f2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin: Determining golden image for each image in the test set #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "128ec93e-6f5b-4abe-b832-e7af950ba281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall - test_data is a list of tuples: [((img_path1, img_path2), label), ...]\n",
    "# and labels are either 0 (negative pair) or 1 (positive pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c87f72a-c375-4f30-aae4-3efeedfb9296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Collect all unique images from test_data\n",
    "test_paths_set = set()\n",
    "for (img_path1, img_path2), label in test_data:\n",
    "    test_paths_set.update([img_path1, img_path2])\n",
    "\n",
    "candidate_image_paths  = list(test_paths_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ee7aa0d7-09d1-47e4-b831-96cc2c89bc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "530"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_paths_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2981b916-c390-4b6c-9af5-de3f591327e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/cs/labs/tomhope/yuvalbus/pmc/pythonProject/data2/PMC6336654/6336654_1/6336654_1_5.jpg',\n",
       " '/cs/labs/tomhope/yuvalbus/pmc/pythonProject/data2/PMC8211554/8211554_1/8211554_1_1.jpg',\n",
       " '/cs/labs/tomhope/yuvalbus/pmc/pythonProject/data2/PMC5438232/5438232_1/5438232_1_4.jpg',\n",
       " '/cs/labs/tomhope/yuvalbus/pmc/pythonProject/data2/PMC2577102/2577102_1/2577102_1_3.jpg',\n",
       " '/cs/labs/tomhope/yuvalbus/pmc/pythonProject/data2/PMC4557154/4557154_1/4557154_1_2.jpg']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_image_paths[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65f6e252-8672-4b56-a189-f8bd7308338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create the ground_truth mapping\n",
    "ground_truth = {}\n",
    "\n",
    "for (img_path1, img_path2), label in test_data:\n",
    "    if label == 1:\n",
    "        if img_path1 not in ground_truth:\n",
    "            ground_truth[img_path1] = img_path2\n",
    "        if img_path2 not in ground_truth:\n",
    "            ground_truth[img_path2] = img_path1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "142f7062-b345-403c-aeab-f23170c14d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "530"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28d8e3a0-a60d-4882-85e3-673fbdfb9eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbeddingModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ImageEmbeddingModel, self).__init__()\n",
    "        self.model = model \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use encode_image to get the embeddings\n",
    "        embeddings = self.model.encode_image(x)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50ba3d46-419a-462d-88ee-46774c7878f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to check Retrieval Metrics for the original BiomedCLIP\n",
    "image_embedding_model = ImageEmbeddingModel(biomedclip_model)\n",
    "image_embedding_model.to(device)\n",
    "image_embedding_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6dfc688f-945b-406c-b634-7b59b7a7baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reload the trained model\n",
    "# base_path = '/cs/labs/tomhope/yuvalbus/pmc/pythonProject/largeListsGuy'\n",
    "# image_embedding_model = torch.load(base_path+'/image_embedding_model_full.pth')\n",
    "# image_embedding_model.to(device)\n",
    "# image_embedding_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19cb1c25-2513-4674-995b-cb80ade948ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset class for candidate images\n",
    "class CandidateImageDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return img_path, image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "520dfa8d-3d6b-445e-8af3-4bf57d49ee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a candidate dataset\n",
    "candidate_dataset = CandidateImageDataset(candidate_image_paths, transform=preprocess)\n",
    "\n",
    "# Create a candidate dataloader\n",
    "candidate_dataloader = DataLoader(candidate_dataset, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f7713ce-a829-4b9b-aabb-318d81f072d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute candidate embeddings\n",
    "candidate_embeddings = {}\n",
    "with torch.no_grad():\n",
    "    for img_paths, images in candidate_dataloader:\n",
    "        images = images.to(device)\n",
    "        embeddings = image_embedding_model(images)\n",
    "        embeddings = embeddings.cpu().numpy()\n",
    "        for img_path, embedding in zip(img_paths, embeddings):\n",
    "            candidate_embeddings[img_path] = embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ac6edf4-ad5d-42d9-a05d-0f057c5daf4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "530"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidate_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "221f9e3c-f92d-42eb-bc32-5c7667f4cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset class for query images\n",
    "class QueryImageDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return img_path, image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45f665e9-8c26-4f8a-82de-e2c24870a4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_image_paths = list(ground_truth.keys())\n",
    "\n",
    "# Create a query dataset\n",
    "query_dataset = QueryImageDataset(query_image_paths, transform=preprocess)\n",
    "\n",
    "# Create a query dataloader\n",
    "query_dataloader = DataLoader(query_dataset, batch_size=64, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1f72d05-ead9-451e-a0ff-1fb576cea664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute query embeddings\n",
    "query_embeddings = {}\n",
    "with torch.no_grad():\n",
    "    for img_paths, images in query_dataloader:\n",
    "        images = images.to(device)\n",
    "        embeddings = image_embedding_model(images)\n",
    "        embeddings = embeddings.cpu().numpy()\n",
    "        for img_path, embedding in zip(img_paths, embeddings):\n",
    "            query_embeddings[img_path] = embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b2306a9-c429-4e95-aad5-bc6840e1fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval Evaluation with Exclusion of Query Images\n",
    "\n",
    "hits_at_k = {1: 0, 3: 0, 5: 0}\n",
    "reciprocal_ranks = []\n",
    "num_queries = len(query_embeddings)\n",
    "\n",
    "# To ensure the alignment of the keys with their values\n",
    "candidate_ids = list(candidate_embeddings.keys())\n",
    "candidate_emb_matrix = np.array([candidate_embeddings[cid] for cid in candidate_ids])\n",
    "\n",
    "for query_id, query_emb in query_embeddings.items():\n",
    "    # Exclude the query image from the candidate set\n",
    "    adjusted_candidate_ids = [cid for cid in candidate_ids if cid != query_id]\n",
    "    adjusted_candidate_emb_matrix = np.array([candidate_embeddings[cid] for cid in adjusted_candidate_ids])\n",
    "\n",
    "    # Compute similarities between the query and adjusted candidates\n",
    "    query_emb_vector = np.expand_dims(query_emb, axis=0) # Shape (1, 512)\n",
    "    similarities = cosine_similarity(query_emb_vector, adjusted_candidate_emb_matrix)[0] # (1, 512) x (512, 5112) -> (1, 5112)                                  \n",
    "\n",
    "    # Pair adjusted_cadidate lists with similarities\n",
    "    similarities_ids_pair_list = list(zip(adjusted_candidate_ids, similarities))\n",
    "\n",
    "    # Sort in descending order the similarities of the query with respect to each candidate\n",
    "    ranked_similarities = sorted(similarities_ids_pair_list, key=lambda x: x[1], reverse=True)\n",
    "    ranked_candidate_ids = [candidate_id for candidate_id, _ in ranked_similarities]\n",
    "\n",
    "    # Get the golden image for the query\n",
    "    golden_image = ground_truth[query_id]\n",
    "\n",
    "    # Compute Hits@K\n",
    "    for K in hits_at_k:\n",
    "        if golden_image in ranked_candidate_ids[:K]:\n",
    "            hits_at_k[K] += 1\n",
    "\n",
    "    # Compute Reciprocal Rank\n",
    "    if golden_image in ranked_candidate_ids:\n",
    "        golden_img_idx = ranked_candidate_ids.index(golden_image)\n",
    "        golden_img_rank = golden_img_idx + 1\n",
    "        reciprocal_ranks.append(1.0 / golden_img_rank)\n",
    "    else:\n",
    "        reciprocal_ranks.append(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76f25ad2-0af4-4344-85e8-81c43053090d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits@1: 0.4283\n",
      "Hits@3: 0.6472\n",
      "Hits@5: 0.7472\n",
      "MRR: 0.5702\n"
     ]
    }
   ],
   "source": [
    "# Compute and Display Metrics\n",
    "for K in hits_at_k:\n",
    "    hits_at_k[K] = hits_at_k[K] / num_queries\n",
    "    print(f\"Hits@{K}: {hits_at_k[K]:.4f}\")\n",
    "\n",
    "reciprocal_ranks_sum = sum(reciprocal_ranks)\n",
    "mrr = reciprocal_ranks_sum / num_queries\n",
    "print(f\"MRR: {mrr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f10b4e-118b-4ee3-83cd-548386c9cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hits@1: 0.1396\n",
    "Hits@3: 0.2775\n",
    "Hits@5: 0.3633\n",
    "MRR: 0.2542\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97f768f-96d3-4924-955e-459bfed1a2cf",
   "metadata": {},
   "source": [
    "CURRENTLY THE BEST.\n",
    "\n",
    "Before Fine-Tuning pos > 0.86, 0.7<vis_neg<0.8 in range(1, 130), no augmentation:\n",
    "\n",
    "Hits@1: 0.2321\n",
    "\n",
    "Hits@3: 0.4291\n",
    "\n",
    "Hits@5: 0.5218\n",
    "\n",
    "MRR: 0.3706\n",
    "\n",
    "After Fine-Tuning:\n",
    "\n",
    "Adam:\n",
    "\n",
    "RMSprop:\n",
    "\n",
    "SGD:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06babc2f-7eba-4e7f-9711-3ab53ed1313e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbd02eec-f10a-4a2a-b165-c44f6800b06a",
   "metadata": {},
   "source": [
    "Before Fine-Tuning best f1-score model:\n",
    "\n",
    "Hits@1: 0.1429\n",
    "\n",
    "Hits@3: 0.2857\n",
    "\n",
    "Hits@5: 0.3829\n",
    "\n",
    "MRR: 0.2589\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdbcb36-7d80-4608-8a7d-50a9e51b17b8",
   "metadata": {},
   "source": [
    "Before Fine-Tuning pos > 0.9, 0.55<neg<0.65 & vis_neg > 0.7 in range(1, 130):\n",
    "\n",
    "Hits@1: 0.2978\n",
    "\n",
    "Hits@3: 0.5393\n",
    "\n",
    "Hits@5: 0.6461\n",
    "\n",
    "MRR: 0.4553\n",
    "\n",
    "After Fine-Tuning pos > 0.9, 0.55<neg<0.65 & vis_neg > 0.7 in range(1, 130):\n",
    "\n",
    "This is after 1 epoch, it got worse afterwards:\n",
    "\n",
    "MRR: 0.458105\n",
    "\n",
    "Hits@1: 0.3146\n",
    "\n",
    "Hits@3: 0.5337\n",
    "\n",
    "Hits@5: 0.6292"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d3b8ee-fa41-4a5e-aa6b-f019b6de07ce",
   "metadata": {},
   "source": [
    "Before Fine-Tuning pos > 0.8, 0.55<neg<0.65 & vis_neg > 0.7 in range(1, 130):\n",
    "\n",
    "Hits@1: 0.0909\n",
    "\n",
    "Hits@3: 0.1878\n",
    "\n",
    "Hits@5: 0.2695\n",
    "\n",
    "MRR: 0.1866\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f31ffd-8de1-4e1c-accd-6b79d6d59ab6",
   "metadata": {},
   "source": [
    "Before Fine-Tuning pos > 0.9, 0.60<neg<0.65 & 0.7 < vis_neg < 0.8 in range(1, 130):\n",
    "MRR: 0.3751\n",
    "Hits@1: 0.2188\n",
    "Hits@3: 0.4467\n",
    "Hits@5: 0.5515\n",
    "\n",
    "After Fine-Tuning pos > 0.9, 0.60<neg<0.65 & 0.7 < vis_neg < 0.8 in range(1, 130):\n",
    "MRR: 0.3781\n",
    "Hits@1: 0.2316\n",
    "Hits@3: 0.4375\n",
    "Hits@5: 0.5368"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c44b2f-14f5-4f01-abdc-555b13af6f39",
   "metadata": {},
   "source": [
    "Before Fine-Tuning pos > 0.9, 0.55<neg<0.65 & 0.7<vis_neg<0.8 in range(1, 130):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c8040c-0a5f-47ef-91b7-753a9d1c2032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8462b6-29f9-4cd1-aef9-351f14258718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d346e64-4cb8-40b7-8b44-2bd4d4af830c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4988b6-7619-4e20-824b-8653c67e10d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End: Determining golden image for each image in the test set #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5cc6f6a2-bccf-44ed-9808-6eb5f4b8c5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    <function _convert_to_rgb at 0x7f3c39fcb240>\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7de114c7-6ddc-4e0d-b685-5fb35e967c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEmbeddingModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ImageEmbeddingModel, self).__init__()\n",
    "        self.model = model \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use encode_image to get the embeddings\n",
    "        embeddings = self.model.encode_image(x)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e342d308-15a3-459d-b484-f94f6e7b6112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "image_embedding_model = ImageEmbeddingModel(biomedclip_model)\n",
    "\n",
    "# Set model to train mode and move to device\n",
    "image_embedding_model.train()\n",
    "image_embedding_model.to(device)\n",
    "# Ensure parameters are trainable\n",
    "for param in image_embedding_model.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e6185d23-6981-4310-a17b-a3742acadf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FocalContrastiveLoss(nn.Module):\n",
    "#     def __init__(self, margin, gamma=2.0):\n",
    "#         super(FocalContrastiveLoss, self).__init__()\n",
    "#         self.margin = margin\n",
    "#         self.gamma = gamma\n",
    "\n",
    "#     def forward(self, embedding1, embedding2, label):\n",
    "#         cos_sim = nn.functional.cosine_similarity(embedding1, embedding2)\n",
    "#         cos_dist = 1 - cos_sim\n",
    "#         pos_loss = label * cos_dist.pow(2) * (1 - cos_sim).pow(self.gamma)\n",
    "#         neg_loss = (1 - label) * nn.functional.relu(self.margin - cos_dist).pow(2) * cos_sim.pow(self.gamma)\n",
    "#         loss = pos_loss + neg_loss\n",
    "#         return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29105b65-5d88-4ab6-9c49-ac29dada8e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# margin = 0.7\n",
    "# criterion = FocalContrastiveLoss(margin=margin)\n",
    "\n",
    "# # 6. Optimizer\n",
    "# optimizer = torch.optim.Adam(image_embedding_model.parameters(), lr=5e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0f2f19e-44a8-4c46-baea-e5eb8027fb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Loss Function\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin, pos_weight):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def forward(self, embedding1, embedding2, label):\n",
    "        cos_sim = nn.functional.cosine_similarity(embedding1, embedding2)\n",
    "        cos_dist = 1 - cos_sim\n",
    "        # loss = label * cos_dist.pow(2) + \\\n",
    "        #        (1 - label) * nn.functional.relu(self.margin - cos_dist).pow(2)\n",
    "        loss = self.pos_weight * label * cos_dist.pow(2) + \\\n",
    "       (1 - label) * nn.functional.relu(self.margin - cos_dist).pow(2)\n",
    "        return loss.mean()\n",
    "\n",
    "margin = 0.55\n",
    "criterion = ContrastiveLoss(margin=margin, pos_weight=9)\n",
    "\n",
    "# 6. Optimizer\n",
    "optimizer = torch.optim.Adam(image_embedding_model.parameters(), lr=2e-7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f543537-2760-4ee7-88a2-583fd8b68be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FocalContrastiveLoss(nn.Module):\n",
    "#     def __init__(self, margin, gamma=2.0):\n",
    "#         super(FocalContrastiveLoss, self).__init__()\n",
    "#         self.margin = margin\n",
    "#         self.gamma = gamma\n",
    "\n",
    "#     def forward(self, embedding1, embedding2, label):\n",
    "#         cos_sim = nn.functional.cosine_similarity(embedding1, embedding2)\n",
    "#         cos_dist = 1 - cos_sim\n",
    "\n",
    "#         # Calculate focal factors\n",
    "#         pos_focal = (1 - cos_sim).pow(self.gamma)\n",
    "#         neg_focal = cos_sim.pow(self.gamma)\n",
    "\n",
    "#         # Compute losses\n",
    "#         pos_loss = label * pos_focal * cos_dist.pow(2)\n",
    "#         neg_loss = (1 - label) * neg_focal * nn.functional.relu(self.margin - cos_dist).pow(2)\n",
    "\n",
    "#         loss = pos_loss + neg_loss\n",
    "#         return loss.mean()\n",
    "        \n",
    "# margin = 0.7\n",
    "# gamma = 3.0  # Start with gamma = 2.0\n",
    "# criterion = FocalContrastiveLoss(margin=margin, gamma=gamma)\n",
    "\n",
    "# # 6. Optimizer\n",
    "# optimizer = torch.optim.Adam(image_embedding_model.parameters(), lr=5e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc121122-fee9-473b-a638-6f5799867e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FocalContrastiveLoss(nn.Module):\n",
    "#     def __init__(self, margin, gamma=2.0, pos_weight=1.0):\n",
    "#         super(FocalContrastiveLoss, self).__init__()\n",
    "#         self.margin = margin\n",
    "#         self.gamma = gamma\n",
    "#         self.pos_weight = pos_weight\n",
    "\n",
    "#     def forward(self, embedding1, embedding2, label):\n",
    "#         cos_sim = nn.functional.cosine_similarity(embedding1, embedding2)\n",
    "#         cos_dist = 1 - cos_sim\n",
    "\n",
    "#         # Calculate focal factors\n",
    "#         pos_focal = (1 - cos_sim).pow(self.gamma)\n",
    "#         neg_focal = cos_sim.pow(self.gamma)\n",
    "\n",
    "#         # Compute losses with pos_weight applied to positive loss\n",
    "#         pos_loss = self.pos_weight * label * pos_focal * cos_dist.pow(2)\n",
    "#         neg_loss = (1 - label) * neg_focal * nn.functional.relu(self.margin - cos_dist).pow(2)\n",
    "\n",
    "#         loss = pos_loss + neg_loss\n",
    "#         return loss.mean()\n",
    "        \n",
    "# margin = 0.5\n",
    "# gamma = 3.0  \n",
    "# pos_weight=8\n",
    "\n",
    "# criterion = FocalContrastiveLoss(margin=margin, gamma=gamma, pos_weight=pos_weight)\n",
    "\n",
    "# # 6. Optimizer\n",
    "# optimizer = torch.optim.Adam(image_embedding_model.parameters(), lr=5e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c10c1960-3f25-4b17-a2ff-ceb97236cc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Training Loss: 0.219748\n",
      "Epoch [1/5], Test Loss: 0.209443\n",
      "Epoch [2/5], Training Loss: 0.158338\n",
      "Epoch [2/5], Test Loss: 0.217761\n",
      "Epoch [3/5], Training Loss: 0.120097\n",
      "Epoch [3/5], Test Loss: 0.246501\n",
      "Epoch [4/5], Training Loss: 0.087535\n",
      "Epoch [4/5], Test Loss: 0.301915\n",
      "Epoch [5/5], Training Loss: 0.061370\n",
      "Epoch [5/5], Test Loss: 0.345229\n"
     ]
    }
   ],
   "source": [
    "# 7. Training Loop\n",
    "num_epochs = 12\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "all_cos_sims = []\n",
    "all_labels = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    image_embedding_model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for (img1, img2), labels in train_dataloader:\n",
    "        img1 = img1.to(device)\n",
    "        img2 = img2.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        embeddings1 = image_embedding_model(img1)\n",
    "        embeddings2 = image_embedding_model(img2)\n",
    "\n",
    "        loss = criterion(embeddings1, embeddings2, labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.6f}')\n",
    "\n",
    "    # Evaluate on the test set after each epoch\n",
    "    image_embedding_model.eval()  # Set model to evaluation mode\n",
    "    total_samples = 0\n",
    "    test_loss = 0\n",
    "    all_cos_sims = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for (img1, img2), labels in test_dataloader:\n",
    "            img1 = img1.to(device)\n",
    "            img2 = img2.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            embeddings1 = image_embedding_model(img1)\n",
    "            embeddings2 = image_embedding_model(img2)\n",
    "\n",
    "            loss = criterion(embeddings1, embeddings2, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate cosine similarity\n",
    "            cos_sim = nn.functional.cosine_similarity(embeddings1, embeddings2)\n",
    "            \n",
    "            # Collect all cosine similarities and labels\n",
    "            all_cos_sims.extend(cos_sim.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_dataloader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Test Loss: {avg_test_loss:.6f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d539f423-5772-4c92-93a1-647270535463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Threshold: 0.6327815055847168\n",
      "Confusion Matrix:\n",
      "[[2018  224]\n",
      " [ 153  351]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9295    0.9001    0.9146      2242\n",
      "         1.0     0.6104    0.6964    0.6506       504\n",
      "\n",
      "    accuracy                         0.8627      2746\n",
      "   macro avg     0.7700    0.7983    0.7826      2746\n",
      "weighted avg     0.8710    0.8627    0.8661      2746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the optimal threshold based on precision-recall curve\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(all_labels, all_cos_sims)\n",
    "f1_scores = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f\"Optimal Threshold: {optimal_threshold}\")\n",
    "\n",
    "\n",
    "# Apply optimal threshold to generate predictions\n",
    "all_predictions = (torch.tensor(all_cos_sims) > optimal_threshold).float()\n",
    "\n",
    "# Generate classification metrics\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "report = classification_report(all_labels, all_predictions, digits=4)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8af470-ddfd-4472-8101-ca450b434c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae215639-b59f-4662-932b-768f7a28ef82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20791f2-a2d9-4fb9-9409-1e93bb1d8655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc2821a7-582a-4dd9-8642-6297d3c6fcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Threshold for F1 Score: 0.7050\n",
      "Confusion Matrix at Optimal Threshold:\n",
      "[[2020  222]\n",
      " [ 159  345]]\n",
      "Classification Report at Optimal Threshold:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9270    0.9010    0.9138      2242\n",
      "         1.0     0.6085    0.6845    0.6443       504\n",
      "\n",
      "    accuracy                         0.8613      2746\n",
      "   macro avg     0.7677    0.7928    0.7790      2746\n",
      "weighted avg     0.8686    0.8613    0.8643      2746\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NO FINE-TUNING\n",
    "from sklearn.metrics import precision_recall_curve, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Collect all true labels and cosine similarities\n",
    "all_labels = []\n",
    "all_cos_sims = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (img1, img2), labels in test_dataloader:\n",
    "        img1 = img1.to(device)\n",
    "        img2 = img2.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        embeddings1 = biomedclip_model.encode_image(img1)\n",
    "        embeddings2 = biomedclip_model.encode_image(img2)\n",
    "\n",
    "        cos_sim = nn.functional.cosine_similarity(embeddings1, embeddings2)\n",
    "        \n",
    "        all_cos_sims.extend(cos_sim.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert lists to arrays for compatibility\n",
    "all_cos_sims = np.array(all_cos_sims)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# Compute precision, recall, and F1 score across different thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(all_labels, all_cos_sims)\n",
    "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-6)  # Avoid division by zero\n",
    "\n",
    "# Find the threshold that gives the highest F1 score\n",
    "optimal_idx = np.argmax(f1_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "print(f\"Optimal Threshold for F1 Score: {optimal_threshold:.4f}\")\n",
    "\n",
    "# Now, using the optimal threshold to evaluate performance\n",
    "optimal_predictions = (all_cos_sims > optimal_threshold).astype(float)\n",
    "\n",
    "# Compute confusion matrix and classification report\n",
    "cm = confusion_matrix(all_labels, optimal_predictions)\n",
    "print(\"Confusion Matrix at Optimal Threshold:\")\n",
    "print(cm)\n",
    "\n",
    "report = classification_report(all_labels, optimal_predictions, digits=4)\n",
    "print(\"Classification Report at Optimal Threshold:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb566504-3649-4e41-879e-75c3e0ff5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save({\n",
    "#     'model_lreightToMinusSeven_posWeightEight_marginSix_fiveEpochs_augmentation_dict': image_embedding_model.state_dict(),\n",
    "#     'optimizer_lreightToMinusSeven_posWeightEight_MarginSix_fiveEpochs_augmentation_dict': optimizer.state_dict(),\n",
    "# }, 'model_lreightToMinusSeven_posWeightEight_MarginSix_fiveEpochs_augmentation.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d60af71-1b6d-45b3-a14f-5ad17cc65c07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d7572ed-68ec-4325-a34b-d8ad3de1e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the saved dictionary\n",
    "# checkpoint = torch.load('posWeight_lrTenToMinusFour_PredZeroPointEight.pth')\n",
    "\n",
    "# # Create the model instance\n",
    "# image_embedding_model = ImageEmbeddingModel(image_encoder)\n",
    "\n",
    "# # Load the model state dictionary from the checkpoint\n",
    "# image_embedding_model.load_state_dict(checkpoint['model_posWeight_lrTenToMinusFour_PredZeroPointEight_dict'])\n",
    "\n",
    "# # Set the model to evaluation mode\n",
    "# image_embedding_model.eval()\n",
    "# image_embedding_model.to(device)\n",
    "\n",
    "# # If you want to load the optimizer state too, you can initialize the optimizer and load its state\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_posWeight_lrTenToMinusFour_PredZeroPointEight_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4dc77d40-fff0-4248-a4a9-000065e5ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# # Collect all true labels and predictions\n",
    "# all_labels = []\n",
    "# all_predictions = []\n",
    "# cos_sims = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for (img1, img2), labels in test_dataloader:\n",
    "#         img1 = img1.to(device)\n",
    "#         img2 = img2.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         embeddings1 = image_embedding_model(img1)\n",
    "#         embeddings2 = image_embedding_model(img2)\n",
    "\n",
    "#         # Normalize embeddings\n",
    "#         embeddings1_norm = embeddings1 / embeddings1.norm(dim=1, keepdim=True)\n",
    "#         embeddings2_norm = embeddings2 / embeddings2.norm(dim=1, keepdim=True)\n",
    "        \n",
    "#         # Compute cosine similarity per pair\n",
    "#         cos_sim = nn.functional.cosine_similarity(embeddings1_norm, embeddings2_norm)\n",
    "\n",
    "#         # Apply threshold\n",
    "#         predictions = (cos_sim > 0.6).float()\n",
    "\n",
    "#         # Convert to CPU and numpy for metrics\n",
    "#         all_predictions.extend(predictions.cpu().numpy())\n",
    "#         all_labels.extend(labels.cpu().numpy())\n",
    "#         cos_sims.extend(cos_sim.cpu().numpy())\n",
    "\n",
    "# # Compute confusion matrix\n",
    "# cm = confusion_matrix(all_labels, all_predictions)\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(cm)\n",
    "\n",
    "# # Classification report\n",
    "# report = classification_report(all_labels, all_predictions, digits=4)\n",
    "# print(\"Classification Report:\")\n",
    "# print(report)\n",
    "\n",
    "# # Optional: Print cosine similarities if needed\n",
    "# # print(\"Cosine Similarities:\")\n",
    "# # print(cos_sims)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aa14e2-9048-4376-89c4-00b5139a0a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-11-23 11:19:30,391] A new study created in memory with name: no-name-a80077f5-0545-43fb-b14c-fa1a8f6e3ec2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/11], Training Loss: 0.653435\n",
      "Epoch [1/11], F1 Positive: 0.645463\n",
      "Epoch [2/11], Training Loss: 0.648978\n",
      "Epoch [2/11], F1 Positive: 0.646067\n",
      "Epoch [3/11], Training Loss: 0.649246\n",
      "Epoch [3/11], F1 Positive: 0.646067\n",
      "Epoch [4/11], Training Loss: 0.644916\n",
      "Epoch [4/11], F1 Positive: 0.646616\n"
     ]
    }
   ],
   "source": [
    "# using optuna to determine best hyperparameters\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import optuna\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ImageEmbeddingModel(nn.Module):\n",
    "    def __init__(self, model, dropout_rate):\n",
    "        super(ImageEmbeddingModel, self).__init__()\n",
    "        self.model = model \n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use encode_image to get the embeddings\n",
    "        embeddings = self.model.encode_image(x)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "# Function to initialize the model\n",
    "def initialize_biomedclip_model():\n",
    "    # Replace with your actual model initialization code\n",
    "    # Using your provided code\n",
    "    biomedclip_model, preprocess = create_model_from_pretrained('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "    biomedclip_model.to(device)\n",
    "    tokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "    return biomedclip_model, preprocess, tokenizer\n",
    "\n",
    "# Define a callback function to log the best trial so far\n",
    "def log_and_save_best_trial(study, trial):\n",
    "    if study.best_trial == trial:\n",
    "        # Log the best trial so far\n",
    "        print(f\"[{trial.datetime_start}] New best trial: Trial {trial.number}\")\n",
    "        print(f\"  F1-Score: {trial.value}\")\n",
    "        print(f\"  Parameters: {trial.params}\")\n",
    "\n",
    "        # Save the best trial to a file\n",
    "        with open(\"best_trial_so_far.txt\", \"w\") as f:\n",
    "            f.write(f\"Best trial so far: Trial {trial.number}\\n\")\n",
    "            f.write(f\"  F1-Score: {trial.value}\\n\")\n",
    "            for key, value in trial.params.items():\n",
    "                f.write(f\"  {key}: {value}\\n\")\n",
    "\n",
    "# Define the Focal Contrastive Loss\n",
    "class FocalContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin, gamma=2.0, pos_weight=1.0):\n",
    "        super(FocalContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.gamma = gamma\n",
    "        self.pos_weight = pos_weight\n",
    "        \n",
    "    def forward(self, embedding1, embedding2, label):\n",
    "        cos_sim = F.cosine_similarity(embedding1, embedding2)\n",
    "        cos_dist = 1 - cos_sim\n",
    "\n",
    "        pos_focal = (1 - cos_sim).pow(self.gamma)\n",
    "        neg_focal = cos_sim.pow(self.gamma)\n",
    "\n",
    "        pos_loss = self.pos_weight * label * pos_focal * cos_dist.pow(2)\n",
    "        neg_loss = (1 - label) * neg_focal * F.relu(self.margin - cos_dist).pow(2)\n",
    "\n",
    "        loss = pos_loss + neg_loss\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# Define the Optuna objective function with discrete options and pruning\n",
    "def objective(trial):\n",
    "    # Set random seed for reproducibility\n",
    "    seed = 42\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Suggest discrete hyperparameters based on previous results\n",
    "    margin = trial.suggest_categorical('margin', [0.5, 0.55])\n",
    "    pos_weight = trial.suggest_categorical('pos_weight', [6.0, 8.0, 11.0])\n",
    "    learning_rate = trial.suggest_categorical('lr', [1.2e-07, 1.5e-07, 1.8e-07, 2.2e-07, 2.5e-07, 3.5e-07])\n",
    "    gamma = trial.suggest_categorical('gamma', [0])\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])\n",
    "    dropout_rate = trial.suggest_categorical('dropout_rate', [0.0, 0.1, 0.3])\n",
    "\n",
    "\n",
    "    # Initialize the model within the objective function\n",
    "    biomedclip_model, preprocess, _ = initialize_biomedclip_model()\n",
    "\n",
    "    # Initialize the image embedding model\n",
    "    image_embedding_model = ImageEmbeddingModel(biomedclip_model, dropout_rate=dropout_rate).to(device)\n",
    "\n",
    "    # Data augmentation parameters\n",
    "    def get_augmentation(trial):\n",
    "        rotation_degree = trial.suggest_categorical('rotation_degree', [0, 10, 15])\n",
    "        horizontal_flip = trial.suggest_categorical('horizontal_flip', [True, False])\n",
    "        vertical_flip = trial.suggest_categorical('vertical_flip', [False])  # Generally avoid vertical flip in medical images\n",
    "        brightness = trial.suggest_categorical('brightness', [0.0, 0.1, 0.2])\n",
    "        contrast = trial.suggest_categorical('contrast', [0.0, 0.1, 0.2])\n",
    "        saturation = trial.suggest_categorical('saturation', [0.0, 0.1, 0.2])\n",
    "        hue = trial.suggest_categorical('hue', [0.0, 0.05, 0.1])\n",
    "\n",
    "        transform_list = []\n",
    "\n",
    "        if horizontal_flip:\n",
    "            transform_list.append(transforms.RandomHorizontalFlip(p=0.5))\n",
    "        if vertical_flip:\n",
    "            transform_list.append(transforms.RandomVerticalFlip(p=0.5))\n",
    "        if rotation_degree > 0:\n",
    "            transform_list.append(transforms.RandomRotation(degrees=rotation_degree))\n",
    "        if any([brightness > 0.0, contrast > 0.0, saturation > 0.0, hue > 0.0]):\n",
    "            transform_list.append(transforms.ColorJitter(\n",
    "                brightness=brightness,\n",
    "                contrast=contrast,\n",
    "                saturation=saturation,\n",
    "                hue=hue))\n",
    "        if transform_list is not None:\n",
    "            transform = transforms.Compose(transform_list)\n",
    "        else:\n",
    "            transform = None\n",
    "            \n",
    "        return transform\n",
    "\n",
    "    positive_augmentation = get_augmentation(trial)\n",
    "\n",
    "    # Dataloader for training set\n",
    "    train_dataset = TrainImagePairDataset(train_data, transform=preprocess, augmentation=positive_augmentation)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    # Dataloader for test set\n",
    "    test_dataset = TestImagePairDataset(test_data, transform=preprocess)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Define the criterion and optimizer\n",
    "    criterion = FocalContrastiveLoss(margin=margin, gamma=gamma, pos_weight=pos_weight)\n",
    "\n",
    "    # Regularization\n",
    "    weight_decay = trial.suggest_categorical('weight_decay', [0.0, 1e-5, 1e-4, 1e-3])\n",
    "\n",
    "    # Initialize optimizer\n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = torch.optim.Adam(image_embedding_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = torch.optim.SGD(image_embedding_model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = torch.optim.RMSprop(image_embedding_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize Learning Rate Scheduler\n",
    "    scheduler_name = trial.suggest_categorical('scheduler', ['None', 'StepLR', 'CosineAnnealingLR'])\n",
    "    # For StepLR rate scheduler\n",
    "    step_size = trial.suggest_int('step_size', 2, 5)\n",
    "    gamma_scheduler = trial.suggest_categorical('gamma_scheduler', [0.1, 0.5])\n",
    "    # Initialize scheduler\n",
    "    if scheduler_name == 'StepLR':\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma_scheduler)\n",
    "    elif scheduler_name == 'CosineAnnealingLR':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    else:\n",
    "        scheduler = None  # No scheduler used\n",
    "\n",
    "\n",
    "    num_epochs = 11  # Adjust as needed\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        image_embedding_model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for (img1, img2), labels in train_dataloader:\n",
    "            img1 = img1.to(device)\n",
    "            img2 = img2.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            embeddings1 = image_embedding_model(img1)\n",
    "            embeddings2 = image_embedding_model(img2)\n",
    "\n",
    "            loss = criterion(embeddings1, embeddings2, labels)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.6f}')\n",
    "        \n",
    "        # In the training loop, at the end of each epoch\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "\n",
    "        # Validation loop\n",
    "        image_embedding_model.eval()\n",
    "        all_cos_sims = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for (img1, img2), labels in test_dataloader:\n",
    "                img1 = img1.to(device)\n",
    "                img2 = img2.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                embeddings1 = image_embedding_model(img1)\n",
    "                embeddings2 = image_embedding_model(img2)\n",
    "\n",
    "                cos_sim = F.cosine_similarity(embeddings1, embeddings2)\n",
    "                all_cos_sims.extend(cos_sim.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Convert to numpy arrays\n",
    "        all_cos_sims = np.array(all_cos_sims)\n",
    "        all_labels = np.array(all_labels)\n",
    "\n",
    "        # Compute F1-score for positive class\n",
    "        precision, recall, thresholds = precision_recall_curve(all_labels, all_cos_sims)\n",
    "        f1_scores = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "        optimal_idx = np.argmax(f1_scores)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "        f1_positive = f1_scores[optimal_idx]\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], F1 Positive: {f1_positive:.6f}')\n",
    "\n",
    "        # Store the optimal threshold\n",
    "        trial.set_user_attr('optimal_threshold', optimal_threshold)\n",
    "\n",
    "        # Report intermediate result to Optuna\n",
    "        trial.report(f1_positive, epoch)\n",
    "\n",
    "        # Check if the trial should be pruned\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    return f1_positive  # Optuna will maximize this value\n",
    "\n",
    "# Create an Optuna study with MedianPruner\n",
    "# The pruner waits until at least 3 trials have fully completed before starting to prune subsequent trials\n",
    "# The pruner ignores pruning for the first 3 epochs of each trial\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=3)\n",
    ")\n",
    "\n",
    "# Optimize the objective function\n",
    "study.optimize(objective, n_trials=50, callbacks=[log_and_save_best_trial])\n",
    "\n",
    "# Display the best hyperparameters\n",
    "print('Best trial:')\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f'  F1-Score (Positive Class): {trial.value}')\n",
    "print('  Best hyperparameters:')\n",
    "for key, value in trial.params.items():\n",
    "    print(f'    {key}: {value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b4597cf-89f9-457f-8c99-33ad09d6e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # using optuna to determine best hyperparameters\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torchvision import transforms\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import f1_score\n",
    "# import optuna\n",
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "# from sklearn.metrics import precision_recall_curve\n",
    "# import random\n",
    "\n",
    "# # Function to initialize the model\n",
    "# def initialize_biomedclip_model():\n",
    "#     # Replace with your actual model initialization code\n",
    "#     # Using your provided code\n",
    "#     biomedclip_model, preprocess = create_model_from_pretrained('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "#     biomedclip_model.to(device)\n",
    "#     tokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "#     return biomedclip_model, preprocess, tokenizer\n",
    "\n",
    "# # Define a callback function to log the best trial so far\n",
    "# def log_and_save_best_trial(study, trial):\n",
    "#     if study.best_trial == trial:\n",
    "#         # Log the best trial so far\n",
    "#         print(f\"[{trial.datetime_start}] New best trial: Trial {trial.number}\")\n",
    "#         print(f\"  F1-Score: {trial.value}\")\n",
    "#         print(f\"  Parameters: {trial.params}\")\n",
    "\n",
    "#         # Save the best trial to a file\n",
    "#         with open(\"best_trial_so_far.txt\", \"w\") as f:\n",
    "#             f.write(f\"Best trial so far: Trial {trial.number}\\n\")\n",
    "#             f.write(f\"  F1-Score: {trial.value}\\n\")\n",
    "#             for key, value in trial.params.items():\n",
    "#                 f.write(f\"  {key}: {value}\\n\")\n",
    "\n",
    "# # Define the Focal Contrastive Loss\n",
    "# class FocalContrastiveLoss(nn.Module):\n",
    "#     def __init__(self, margin, gamma=2.0, pos_weight=1.0):\n",
    "#         super(FocalContrastiveLoss, self).__init__()\n",
    "#         self.margin = margin\n",
    "#         self.gamma = gamma\n",
    "#         self.pos_weight = pos_weight\n",
    "\n",
    "#     def forward(self, embedding1, embedding2, label):\n",
    "#         cos_sim = F.cosine_similarity(embedding1, embedding2)\n",
    "#         cos_dist = 1 - cos_sim\n",
    "\n",
    "#         pos_focal = (1 - cos_sim).pow(self.gamma)\n",
    "#         neg_focal = cos_sim.pow(self.gamma)\n",
    "\n",
    "#         pos_loss = self.pos_weight * label * pos_focal * cos_dist.pow(2)\n",
    "#         neg_loss = (1 - label) * neg_focal * F.relu(self.margin - cos_dist).pow(2)\n",
    "\n",
    "#         loss = pos_loss + neg_loss\n",
    "#         return loss.mean()\n",
    "\n",
    "# # Define the Optuna objective function\n",
    "# def objective(trial):\n",
    "#     # Set random seed for reproducibility\n",
    "#     seed = 42\n",
    "#     torch.manual_seed(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     random.seed(seed)\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "#     # Suggest hyperparameters\n",
    "#     margin = trial.suggest_float('margin', 0.5, 0.6, step=0.05)\n",
    "#     pos_weight = trial.suggest_float('pos_weight', 6.0, 10.0, step=1.0)\n",
    "#     learning_rate = trial.suggest_float('lr', 1e-7, 1e-6, log=True)\n",
    "#     gamma = trial.suggest_int('gamma', 0, 0)\n",
    "\n",
    "#     # Initialize the model within the objective function\n",
    "#     biomedclip_model, preprocess, tokenizer = initialize_biomedclip_model()\n",
    "\n",
    "#     # Initialize the image embedding model\n",
    "#     image_embedding_model = ImageEmbeddingModel(biomedclip_model).to(device)\n",
    "\n",
    "#     # Define the criterion and optimizer\n",
    "#     criterion = FocalContrastiveLoss(margin=margin, gamma=gamma, pos_weight=pos_weight)\n",
    "#     optimizer = torch.optim.Adam(image_embedding_model.parameters(), lr=learning_rate)\n",
    "\n",
    "#     num_epochs = 11  # Adjust as needed\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         image_embedding_model.train()\n",
    "#         train_loss = 0\n",
    "\n",
    "#         for (img1, img2), labels in train_dataloader:\n",
    "#             img1 = img1.to(device)\n",
    "#             img2 = img2.to(device)\n",
    "#             labels = labels.to(device)\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             embeddings1 = image_embedding_model(img1)\n",
    "#             embeddings2 = image_embedding_model(img2)\n",
    "\n",
    "#             loss = criterion(embeddings1, embeddings2, labels)\n",
    "#             train_loss += loss.item()\n",
    "\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#         avg_train_loss = train_loss / len(train_dataloader)\n",
    "#         print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.6f}')\n",
    "\n",
    "#         # Validation loop\n",
    "#         image_embedding_model.eval()\n",
    "#         all_cos_sims = []\n",
    "#         all_labels = []\n",
    "#         with torch.no_grad():\n",
    "#             for (img1, img2), labels in test_dataloader:\n",
    "#                 img1 = img1.to(device)\n",
    "#                 img2 = img2.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "\n",
    "#                 embeddings1 = image_embedding_model(img1)\n",
    "#                 embeddings2 = image_embedding_model(img2)\n",
    "\n",
    "#                 cos_sim = F.cosine_similarity(embeddings1, embeddings2)\n",
    "#                 all_cos_sims.extend(cos_sim.cpu().numpy())\n",
    "#                 all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "#         # Convert to numpy arrays\n",
    "#         all_cos_sims = np.array(all_cos_sims)\n",
    "#         all_labels = np.array(all_labels)\n",
    "\n",
    "#         # Compute F1-score for positive class\n",
    "#         precision, recall, thresholds = precision_recall_curve(all_labels, all_cos_sims)\n",
    "#         f1_scores = 2 * precision * recall / (precision + recall + 1e-6)\n",
    "#         optimal_idx = np.argmax(f1_scores)\n",
    "#         optimal_threshold = thresholds[optimal_idx]\n",
    "#         f1_positive = f1_scores[optimal_idx]\n",
    "#         print(f'Epoch [{epoch+1}/{num_epochs}], F1 Positive: {f1_positive:.6f}')\n",
    "\n",
    "#         # Store the optimal threshold\n",
    "#         trial.set_user_attr('optimal_threshold', optimal_threshold)\n",
    "\n",
    "#         # Report intermediate result to Optuna\n",
    "#         trial.report(f1_positive, epoch)\n",
    "\n",
    "#     return f1_positive  # Optuna will maximize this value\n",
    "\n",
    "# # Create an Optuna study\n",
    "# # study = optuna.create_study(direction='maximize', pruner=optuna.pruners.MedianPruner())\n",
    "# study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# # Optimize the objective function\n",
    "# study.optimize(objective, n_trials=50, callbacks=[log_and_save_best_trial])\n",
    "\n",
    "# # Display the best hyperparameters\n",
    "# print('Best trial:')\n",
    "# trial = study.best_trial\n",
    "\n",
    "# print(f'  F1-Score (Positive Class): {trial.value}')\n",
    "# print('  Best hyperparameters:')\n",
    "# for key, value in trial.params.items():\n",
    "#     print(f'    {key}: {value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c299c2-1686-430c-8cdc-4dce380f0a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain your model on the full training set\n",
    "num_epochs = 10  # Increase the number of epochs for final training\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    image_embedding_model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for (img1, img2), labels in train_dataloader:\n",
    "        img1 = img1.to(device)\n",
    "        img2 = img2.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        embeddings1 = image_embedding_model(img1)\n",
    "        embeddings2 = image_embedding_model(img2)\n",
    "\n",
    "        loss = criterion(embeddings1, embeddings2, labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss/len(train_dataloader):.6f}')\n",
    "\n",
    "# Evaluate on the validation set\n",
    "image_embedding_model.eval()\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "with torch.no_grad():\n",
    "    for (img1, img2), labels in test_dataloader:\n",
    "        img1 = img1.to(device)\n",
    "        img2 = img2.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        embeddings1 = image_embedding_model(img1)\n",
    "        embeddings2 = image_embedding_model(img2)\n",
    "\n",
    "        cos_sim = F.cosine_similarity(embeddings1, embeddings2)\n",
    "        predictions = (cos_sim > 0.5).float()  # Use the threshold that worked best\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Compute final F1-score for positive class\n",
    "all_labels = np.array(all_labels)\n",
    "all_predictions = np.array(all_predictions)\n",
    "f1_positive = f1_score(all_labels, all_predictions, pos_label=1, average='binary')\n",
    "\n",
    "print(f'Final F1-Score for Positive Class: {f1_positive:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
