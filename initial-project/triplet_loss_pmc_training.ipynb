{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import open_clip\n",
    "from open_clip import create_model_from_pretrained, get_tokenizer\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "base_path = '/cs/labs/tomhope/yuvalbus/pmc/pythonProject/largeListsGuy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1056 (79.94%)\n",
      "Test samples: 265 (20.06%)\n",
      "Discarded samples: 0\n",
      "UID exclusivity between training and test sets is maintained.\n",
      "Total Positive Pairs 1321\n",
      "Training Positive Pairs 1056\n",
      "Training Negative Pairs (before the hard visual negatives) 0\n",
      "Test Positive Pairs 265\n",
      "Training Negative Pairs (after the hard visual negatives) 1727\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "with open(base_path + \"/retrieval_labeled_img_pairs.pkl\", \"rb\") as f:\n",
    "    labeled_img_pairs = pickle.load(f)\n",
    "\n",
    "def extract_uid(img_path):\n",
    "    \"\"\"\n",
    "    Extracts the uid from the image path.\n",
    "    Assumes that the uid is the last directory before the image filename.\n",
    "    \"\"\"\n",
    "    # Split the path into parts\n",
    "    path_parts = os.path.normpath(img_path).split(os.sep)\n",
    "    # Get the uid (second last part)\n",
    "    uid = path_parts[-2]\n",
    "    return uid\n",
    "\n",
    "def dfs(uid, visited, component):\n",
    "    visited.add(uid)\n",
    "    component.add(uid)\n",
    "    for neighbor in uid_graph[uid]:\n",
    "        if neighbor not in visited:\n",
    "            dfs(neighbor, visited, component)\n",
    "\n",
    "# Build the uid_graph\n",
    "uid_graph = defaultdict(set)\n",
    "\n",
    "# Build the graph\n",
    "for (img_path1, img_path2), label in labeled_img_pairs:\n",
    "    uid1 = extract_uid(img_path1)\n",
    "    uid2 = extract_uid(img_path2)\n",
    "    uid_graph[uid1].add(uid2)\n",
    "    uid_graph[uid2].add(uid1)\n",
    "\n",
    "# Find connected components\n",
    "visited = set()\n",
    "components = []\n",
    "\n",
    "for uid in uid_graph:\n",
    "    if uid not in visited:\n",
    "        component = set()\n",
    "        dfs(uid, visited, component)\n",
    "        components.append(component)\n",
    "\n",
    "# Step 1: Build UID to component index mapping\n",
    "uid_to_component_idx = {}\n",
    "\n",
    "for idx, component in enumerate(components):\n",
    "    for uid in component:\n",
    "        uid_to_component_idx[uid] = idx\n",
    "\n",
    "# Step 2: Count samples per component\n",
    "component_sample_counts = [0] * len(components)\n",
    "\n",
    "for (img_path1, img_path2), label in labeled_img_pairs:\n",
    "    uid1 = extract_uid(img_path1)\n",
    "    component_idx = uid_to_component_idx[uid1]\n",
    "    component_sample_counts[component_idx] += 1\n",
    "\n",
    "# Step 3: Sort components by sample count\n",
    "components_with_counts = list(zip(components, component_sample_counts))\n",
    "components_with_counts.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Step 4: Assign components to training and test sets\n",
    "total_samples = len(labeled_img_pairs)\n",
    "desired_train_samples = int(total_samples * 0.8)\n",
    "\n",
    "train_uids = set()\n",
    "test_uids = set()\n",
    "\n",
    "accumulated_train_samples = 0\n",
    "\n",
    "for component, sample_count in components_with_counts:\n",
    "    if accumulated_train_samples < desired_train_samples:\n",
    "        train_uids.update(component)\n",
    "        accumulated_train_samples += sample_count\n",
    "    else:\n",
    "        test_uids.update(component)\n",
    "\n",
    "# Step 5: Assign samples to training and test sets, discard cross-set samples\n",
    "train_data = []\n",
    "test_data = []\n",
    "discarded_samples = []\n",
    "\n",
    "for sample in labeled_img_pairs:\n",
    "    (img_path1, img_path2), label = sample\n",
    "    uid1 = extract_uid(img_path1)\n",
    "    uid2 = extract_uid(img_path2)\n",
    "\n",
    "    if uid1 in train_uids and uid2 in train_uids:\n",
    "        train_data.append(sample)\n",
    "    elif uid1 in test_uids and uid2 in test_uids:\n",
    "        test_data.append(sample)\n",
    "    else:\n",
    "        # Discard cross-set samples to maintain UID exclusivity\n",
    "        discarded_samples.append(sample)\n",
    "\n",
    "# Step 6: Verify the split ratio\n",
    "train_sample_count = len(train_data)\n",
    "test_sample_count = len(test_data)\n",
    "total_sample_count = train_sample_count + test_sample_count\n",
    "\n",
    "train_ratio = train_sample_count / total_sample_count\n",
    "test_ratio = test_sample_count / total_sample_count\n",
    "\n",
    "print(f\"Training samples: {train_sample_count} ({train_ratio:.2%})\")\n",
    "print(f\"Test samples: {test_sample_count} ({test_ratio:.2%})\")\n",
    "print(f\"Discarded samples: {len(discarded_samples)}\")\n",
    "\n",
    "# Step 7: Ensure UID exclusivity in the test set\n",
    "uids_in_training_samples = set()\n",
    "for (img_path1, img_path2), label in train_data:\n",
    "    uids_in_training_samples.update([extract_uid(img_path1), extract_uid(img_path2)])\n",
    "\n",
    "uids_in_test_samples = set()\n",
    "for (img_path1, img_path2), label in test_data:\n",
    "    uids_in_test_samples.update([extract_uid(img_path1), extract_uid(img_path2)])\n",
    "\n",
    "overlap_uids = uids_in_test_samples.intersection(uids_in_training_samples)\n",
    "assert len(overlap_uids) == 0, \"Overlap detected between training and test UIDs!\"\n",
    "\n",
    "print(\"UID exclusivity between training and test sets is maintained.\")\n",
    "\n",
    "counter = 0\n",
    "for (img_pair, label) in labeled_img_pairs:\n",
    "    if label == 1:\n",
    "        counter += 1\n",
    "print(f\"Total Positive Pairs {counter}\")\n",
    "\n",
    "\n",
    "counter = 0\n",
    "for (img_pair, label) in train_data:\n",
    "    if label == 1:\n",
    "        counter += 1\n",
    "print(f\"Training Positive Pairs {counter}\")\n",
    "\n",
    "counter = 0\n",
    "for (img_pair, label) in train_data:\n",
    "    if label == 0:\n",
    "        counter += 1\n",
    "print(f\"Training Negative Pairs (before the hard visual negatives) {counter}\")\n",
    "\n",
    "counter = 0\n",
    "for (img_pair, label) in test_data:\n",
    "    if label == 1:\n",
    "        counter += 1\n",
    "print(f\"Test Positive Pairs {counter}\")\n",
    "\n",
    "# Load additional negative pairs\n",
    "with open(base_path + \"/visual_labeled_img_negative_pairs.pkl\", \"rb\") as f:\n",
    "    visual_labeled_img_negative_pairs = pickle.load(f)\n",
    "\n",
    "train_data.extend(visual_labeled_img_negative_pairs)\n",
    "counter = 0\n",
    "for (img_pair, label) in train_data:\n",
    "    if label == 0:\n",
    "        counter += 1\n",
    "print(f\"Training Negative Pairs (after the hard visual negatives) {counter}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Classes\n",
    "class TestImagePairDataset(Dataset):\n",
    "    def __init__(self, pairs_list, transform=None):\n",
    "        self.pairs_list = pairs_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        (img_path1, img_path2), label = self.pairs_list[idx]\n",
    "        img1 = Image.open(img_path1).convert('RGB')\n",
    "        img2 = Image.open(img_path2).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return (img1, img2), label\n",
    "\n",
    "class TrainImagePairDataset(Dataset):\n",
    "    def __init__(self, pairs_list, transform=None):\n",
    "        self.pairs_list = pairs_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        (img_path1, img_path2), label = self.pairs_list[idx]\n",
    "        img1 = Image.open(img_path1).convert('RGB')\n",
    "        img2 = Image.open(img_path2).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return (img1, img2), label\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ImageEmbeddingModel(nn.Module):\n",
    "    def __init__(self, model, dropout_rate):\n",
    "        super(ImageEmbeddingModel, self).__init__()\n",
    "        self.model = model \n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Use encode_image to get the embeddings\n",
    "        embeddings = self.model.encode_image(x)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "# Function to initialize the model\n",
    "def initialize_biomedclip_model():\n",
    "    biomedclip_model, preprocess = create_model_from_pretrained('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "    biomedclip_model.to(device)\n",
    "    tokenizer = get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "    return biomedclip_model, preprocess, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import networkx as nx  # For building graph components\n",
    "\n",
    "class TripletImageDataset(Dataset):\n",
    "    def __init__(self, pair_dataset, transform=None, augmentation=None):\n",
    "        \"\"\"\n",
    "        pair_dataset: A list of (((img_path1, img_path2), label), ...).\n",
    "                      label=1 means img_path1 and img_path2 belong to the same class,\n",
    "                      label=0 means different classes.\n",
    "\n",
    "        transform: Transform applied to all images.\n",
    "        augmentation: Additional augmentation for positive images.\n",
    "\n",
    "        This class:\n",
    "        1) Constructs a graph of images connected by edges where label=1.\n",
    "        2) Finds connected components to identify classes.\n",
    "        3) From these classes, creates triplets (anchor, positive, negative).\n",
    "        \"\"\"\n",
    "\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "\n",
    "        # Extract classes from pairs\n",
    "        self.class_to_images = self._group_images_into_classes(pair_dataset)\n",
    "\n",
    "        # Generate triplets\n",
    "        self.triplets = self._generate_triplets()\n",
    "\n",
    "    def _group_images_into_classes(self, pair_dataset):\n",
    "        # Build a graph where each node is an image path.\n",
    "        # Edges with label=1 connect images of the same class.\n",
    "        G = nx.Graph()\n",
    "        all_images = set()\n",
    "        for (img1, img2), label in pair_dataset:\n",
    "            all_images.add(img1)\n",
    "            all_images.add(img2)\n",
    "            G.add_node(img1)\n",
    "            G.add_node(img2)\n",
    "            if label == 1:\n",
    "                G.add_edge(img1, img2)\n",
    "\n",
    "        # Find connected components - each component is a class\n",
    "        components = list(nx.connected_components(G))\n",
    "\n",
    "        class_to_images = {}\n",
    "        for idx, comp in enumerate(components):\n",
    "            # comp is a set of image paths\n",
    "            class_to_images[idx] = list(comp)\n",
    "\n",
    "        return class_to_images\n",
    "\n",
    "    def _generate_triplets(self):\n",
    "        # We now have class_to_images as {class_id: [img_paths]}\n",
    "        # For each class, we'll create triplets by picking an anchor and positive from this class\n",
    "        # and a negative from a different class.\n",
    "        triplets = []\n",
    "\n",
    "        # Get list of classes\n",
    "        classes = list(self.class_to_images.keys())\n",
    "\n",
    "        # We need at least 2 classes to form a negative example\n",
    "        if len(classes) < 2:\n",
    "            print(\"Not enough classes to form triplets.\")\n",
    "            return triplets\n",
    "\n",
    "        for cls in classes:\n",
    "            img_paths = self.class_to_images[cls]\n",
    "            # Need at least 2 images to form anchor-positive pair\n",
    "            if len(img_paths) < 2:\n",
    "                continue\n",
    "\n",
    "            for anchor_idx, anchor_path in enumerate(img_paths):\n",
    "                # Positive candidates are all other images in the same class\n",
    "                positive_candidates = img_paths[:anchor_idx] + img_paths[anchor_idx+1:]\n",
    "                if not positive_candidates:\n",
    "                    continue\n",
    "                positive_path = random.choice(positive_candidates)\n",
    "\n",
    "                # Choose negative class\n",
    "                negative_cls = random.choice([c for c in classes if c != cls])\n",
    "                negative_path = random.choice(self.class_to_images[negative_cls])\n",
    "\n",
    "                triplets.append((anchor_path, positive_path, negative_path))\n",
    "\n",
    "        return triplets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_path, positive_path, negative_path = self.triplets[idx]\n",
    "\n",
    "        anchor_img = Image.open(anchor_path).convert('RGB')\n",
    "        positive_img = Image.open(positive_path).convert('RGB')\n",
    "        negative_img = Image.open(negative_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            anchor_img = self.transform(anchor_img)\n",
    "            positive_img = self.transform(positive_img)\n",
    "            negative_img = self.transform(negative_img)\n",
    "\n",
    "        # Apply augmentation to the positive image if desired\n",
    "        if self.augmentation:\n",
    "            positive_img = self.augmentation(positive_img)\n",
    "\n",
    "        return anchor_img, positive_img, negative_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "margin = 0.1\n",
    "gamma_loss = 60\n",
    "learning_rate = 3e-7\n",
    "dropout_rate = 0.0\n",
    "num_epochs = 5\n",
    "optimizer_name = 'Adam'\n",
    "batch_size = 16\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# Initialize the model\n",
    "biomedclip_model, preprocess, _ = initialize_biomedclip_model()\n",
    "\n",
    "# Initialize the image embedding model\n",
    "image_embedding_model = ImageEmbeddingModel(biomedclip_model, dropout_rate=dropout_rate).to(device)\n",
    "\n",
    "# Ensure all parameters are trainable\n",
    "for param in image_embedding_model.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader for training set\n",
    "train_dataset = TripletImageDataset(train_data, transform=preprocess)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# Dataloader for test set\n",
    "test_dataset = TripletImageDataset(test_data, transform=preprocess)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.TripletMarginLoss(margin=margin) \n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(image_embedding_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CandidateImageDataset in order to calculate efficiently visual embeddings for Retrieval Metrics\n",
    "class CandidateImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return img_path, image\n",
    "\n",
    "# QueryImageDataset in order to calculate efficiently visual embeddings for Retrieval Metrics\n",
    "class QueryImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return img_path, image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Training Loss: 0.060176\n",
      "Epoch [1/5], MRR: 0.570493\n",
      "Hits@1: 0.4283\n",
      "Hits@3: 0.6491\n",
      "Hits@5: 0.7453\n",
      "Epoch [2/5], Training Loss: 0.035246\n",
      "Epoch [2/5], MRR: 0.570903\n",
      "Hits@1: 0.4283\n",
      "Hits@3: 0.6509\n",
      "Hits@5: 0.7472\n",
      "Epoch [3/5], Training Loss: 0.015603\n",
      "Epoch [3/5], MRR: 0.571211\n",
      "Hits@1: 0.4283\n",
      "Hits@3: 0.6509\n",
      "Hits@5: 0.7491\n",
      "Epoch [4/5], Training Loss: 0.008347\n",
      "Epoch [4/5], MRR: 0.572075\n",
      "Hits@1: 0.4302\n",
      "Hits@3: 0.6509\n",
      "Hits@5: 0.7472\n",
      "Epoch [5/5], Training Loss: 0.003862\n",
      "Epoch [5/5], MRR: 0.572080\n",
      "Hits@1: 0.4302\n",
      "Hits@3: 0.6509\n",
      "Hits@5: 0.7472\n"
     ]
    }
   ],
   "source": [
    "# Training and Validation Loop\n",
    "for epoch in range(num_epochs):\n",
    "    image_embedding_model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    # Now the dataloader returns triplets: anchor, positive, negative\n",
    "    for anchor_img, positive_img, negative_img in train_dataloader:\n",
    "        anchor_img = anchor_img.to(device)\n",
    "        positive_img = positive_img.to(device)\n",
    "        negative_img = negative_img.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        anchor_embeddings = image_embedding_model(anchor_img)\n",
    "        positive_embeddings = image_embedding_model(positive_img)\n",
    "        negative_embeddings = image_embedding_model(negative_img)\n",
    "\n",
    "        # Compute triplet loss\n",
    "        loss = criterion(anchor_embeddings, positive_embeddings, negative_embeddings)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.6f}')\n",
    "\n",
    "    # Validation loop (no changes needed, this remains the same as previously)\n",
    "    # Collect all unique image paths from test_data\n",
    "    test_paths_set = set()\n",
    "    for (img_path1, img_path2), _ in test_data:\n",
    "        test_paths_set.update([img_path1, img_path2])\n",
    "\n",
    "    test_paths_list = list(test_paths_set)\n",
    "\n",
    "    # Prepare candidate dataset and dataloader\n",
    "    candidate_dataset = CandidateImageDataset(test_paths_list, transform=preprocess)\n",
    "    candidate_dataloader = DataLoader(candidate_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    image_embedding_model.eval()\n",
    "    # Compute embeddings for retrieval evaluation\n",
    "    with torch.no_grad():\n",
    "        # Compute candidate embeddings\n",
    "        candidate_embeddings = {}\n",
    "        for img_paths, images in candidate_dataloader:\n",
    "            images = images.to(device)\n",
    "            embeddings = image_embedding_model(images)\n",
    "            embeddings = embeddings.cpu().numpy()\n",
    "            for img_path, embedding in zip(img_paths, embeddings):\n",
    "                candidate_embeddings[img_path] = embedding\n",
    "\n",
    "        # Prepare ground truth\n",
    "        ground_truth = {}\n",
    "        for (img_path1, img_path2), label in test_data:\n",
    "            if label == 1:\n",
    "                if img_path1 not in ground_truth:\n",
    "                    ground_truth[img_path1] = img_path2\n",
    "                if img_path2 not in ground_truth:\n",
    "                    ground_truth[img_path2] = img_path1\n",
    "\n",
    "        # Prepare query embeddings\n",
    "        query_image_paths = list(ground_truth.keys())\n",
    "        query_dataset = QueryImageDataset(query_image_paths, transform=preprocess)\n",
    "        query_dataloader = DataLoader(query_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "        query_embeddings = {}\n",
    "        for img_paths, images in query_dataloader:\n",
    "            images = images.to(device)\n",
    "            embeddings = image_embedding_model(images)\n",
    "            embeddings = embeddings.cpu().numpy()\n",
    "            for img_path, embedding in zip(img_paths, embeddings):\n",
    "                query_embeddings[img_path] = embedding\n",
    "\n",
    "    # Retrieval Evaluation (no changes needed)\n",
    "    hits_at_k = {1: 0, 3: 0, 5: 0}\n",
    "    reciprocal_ranks = []\n",
    "    num_queries = len(query_embeddings)\n",
    "\n",
    "    candidate_ids = list(candidate_embeddings.keys())\n",
    "    candidate_emb_matrix = np.array([candidate_embeddings[cid] for cid in candidate_ids])\n",
    "\n",
    "    for query_id, query_emb in query_embeddings.items():\n",
    "        # Exclude the query image from the candidate set\n",
    "        adjusted_candidate_ids = [cid for cid in candidate_ids if cid != query_id]\n",
    "        adjusted_candidate_emb_matrix = np.array([candidate_embeddings[cid] for cid in adjusted_candidate_ids])\n",
    "\n",
    "        # Compute similarities between the query and adjusted candidates\n",
    "        query_emb_vector = np.expand_dims(query_emb, axis=0)\n",
    "        similarities = cosine_similarity(query_emb_vector, adjusted_candidate_emb_matrix)[0]\n",
    "\n",
    "        # Pair adjusted candidate IDs with similarities\n",
    "        similarities_ids_pair_list = list(zip(adjusted_candidate_ids, similarities))\n",
    "\n",
    "        # Rank candidates by similarity (descending order)\n",
    "        ranked_similarities = sorted(similarities_ids_pair_list, key=lambda x: x[1], reverse=True)\n",
    "        ranked_candidate_ids = [candidate_id for candidate_id, _ in ranked_similarities]\n",
    "\n",
    "        # Get the golden image for the query\n",
    "        golden_image = ground_truth[query_id]\n",
    "\n",
    "        # Compute Hits@K\n",
    "        for K in hits_at_k:\n",
    "            if golden_image in ranked_candidate_ids[:K]:\n",
    "                hits_at_k[K] += 1\n",
    "\n",
    "        # Compute Reciprocal Rank\n",
    "        if golden_image in ranked_candidate_ids:\n",
    "            golden_img_idx = ranked_candidate_ids.index(golden_image)\n",
    "            golden_img_rank = golden_img_idx + 1\n",
    "            reciprocal_ranks.append(1.0 / golden_img_rank)\n",
    "        else:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "\n",
    "    # Compute final metrics\n",
    "    for K in hits_at_k:\n",
    "        hits_at_k[K] = hits_at_k[K] / num_queries\n",
    "\n",
    "    mrr = sum(reciprocal_ranks) / num_queries\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], MRR: {mrr:.6f}')\n",
    "    print(f'Hits@1: {hits_at_k[1]:.4f}')\n",
    "    print(f'Hits@3: {hits_at_k[3]:.4f}')\n",
    "    print(f'Hits@5: {hits_at_k[5]:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pmc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
